# 長期保有型パラメータ最適化結果の分析資料

## 1. 最適化の実行概要

### 実行環境
- **実行日**: 2026-01-09
- **最適化方法**: Optuna（TPE Sampler）
- **試行回数**: 200試行（各候補）
- **データ期間**: 2020-01-01 ～ 2025-12-31
- **学習期間**: 2020-01-01 ～ 2022-12-31（train_end_date="2022-12-31"）
- **テスト期間**: 2023-01-01 ～ 2025-12-31

### データリーク対策
- ✅ `require_full_horizon=True`: ホライズン未達の期間を除外
- ✅ `as_of_date`を明示的に指定（DB MAX(date)は使用しない）
- ✅ 時系列分割（train_end_dateを使用）
- ✅ 価格データの物理的な切り取り（`WHERE date <= as_of_date`）
- ✅ 財務データの`disclosed_date <= rebalance_date`フィルタ

### 最適化された候補
1. **operational_24M**: 24ヶ月ホライズン、運用候補
2. **12M_momentum**: 12ヶ月ホライズン、momentum向け
3. **12M_reversal**: 12ヶ月ホライズン、reversal向け

---

## 2. 各候補の最適化結果

### 2.1 operational_24M（24ヶ月ホライズン）

**最適化設定:**
- Study Type: C（広範囲探索）
- リバランス日範囲: 2020-01-01 ～ 2023-12-31（24Mホライズン完走のため）
- 評価の打ち切り日: 2025-12-31

**パフォーマンス:**
| 指標 | 学習データ | テストデータ | 差 |
|------|-----------|------------|-----|
| 年率超過リターン（平均） | 8.78% | 10.22% | -1.44% |
| 年率リターン（平均） | - | 24.93% | - |
| 勝率 | - | 100% (12/12) | - |
| ポートフォリオ数 | - | 12 | - |
| 平均保有期間 | - | 2.00年 | - |

**戦略モード:**
- **判定**: momentum
- RSI: `base=16.61` → `max=59.04`（順張り）
- BB: `base=1.47` → `max=0.73`（逆張り）
- 重み: RSI=0.66, BB=0.34
- **判定理由**: RSIの重みが高く、RSIが順張り方向

**評価:**
- ✅ テストデータで学習データを上回る（過学習なし）
- ✅ 勝率100%（12/12）
- ⚠️ サンプル数が少ない（12件のみ）

---

### 2.2 12M_momentum（12ヶ月ホライズン）

**最適化設定:**
- Study Type: A（BB寄り・低ROE閾値、momentum向け）
- リバランス日範囲: 2020-01-01 ～ 2025-12-31
- 評価の打ち切り日: 2025-12-31

**パフォーマンス:**
| 指標 | 学習データ | テストデータ | 差 |
|------|-----------|------------|-----|
| 年率超過リターン（平均） | 6.83% | 4.37% | +2.46% |
| 年率超過リターン（中央値） | - | 6.43% | - |
| 年率リターン（平均） | - | 21.16% | - |
| 勝率 | - | 66.67% (24/36) | - |
| ポートフォリオ数 | - | 36 | - |
| 平均保有期間 | - | 1.00年 | - |

**戦略モード:**
- **判定**: momentum（修正後）
- RSI: `base=78.85` → `max=47.26`（逆張り）
- BB: `base=-1.25` → `max=0.08`（順張り）
- 重み: RSI=0.23, BB=0.77
- **判定理由**: BBの重みが高く（0.77）、BBが順張り方向

**評価:**
- ✅ 学習とテストの差が小さい（過学習の可能性は低い）
- ✅ 勝率66.67%
- ⚠️ RSIは逆張りだが、BBの重みが高いためmomentumと判定

---

### 2.3 12M_reversal（12ヶ月ホライズン）

**最適化設定:**
- Study Type: B（Value寄り・ROE閾値やや高め、reversal向け）
- リバランス日範囲: 2020-01-01 ～ 2025-12-31
- 評価の打ち切り日: 2025-12-31

**パフォーマンス:**
| 指標 | 学習データ | テストデータ | 差 |
|------|-----------|------------|-----|
| 年率超過リターン（平均） | 12.86% | 1.36% | +11.50% |
| 年率超過リターン（中央値） | - | -1.51% | - |
| 年率リターン（平均） | - | 18.16% | - |
| 勝率 | - | 45.83% (11/24) | - |
| ポートフォリオ数 | - | 36（評価成功24、12件がホライズン未達で除外） | - |
| 平均保有期間 | - | 1.00年 | - |

**戦略モード:**
- **判定**: momentum（修正後）
- RSI: `base=44.79` → `max=57.84`（順張り）
- BB: `base=-0.67` → `max=0.65`（順張り）
- 重み: RSI=0.63, BB=0.37
- **判定理由**: RSIもBBも順張り方向

**評価:**
- ⚠️ **過学習の可能性が高い**（学習12.86% vs テスト1.36%、差+11.50%）
- ⚠️ **勝率が低い**（45.83%、50%未満）
- ⚠️ **Study B（reversal向け）で最適化したが、結果がmomentum寄り**

---

## 3. 問題点の整理

### 3.1 過学習の可能性

| 候補 | 学習 | テスト | 差 | 評価 |
|------|------|--------|-----|------|
| operational_24M | 8.78% | 10.22% | -1.44% | ✅ 過学習なし |
| 12M_momentum | 6.83% | 4.37% | +2.46% | ✅ 過学習の可能性は低い |
| 12M_reversal | 12.86% | 1.36% | +11.50% | ⚠️ **過学習の可能性が高い** |

### 3.2 勝率

| 候補 | 勝率 | 評価 |
|------|------|------|
| operational_24M | 100% (12/12) | ✅ 優秀 |
| 12M_momentum | 66.67% (24/36) | ✅ 良好 |
| 12M_reversal | 45.83% (11/24) | ⚠️ **50%未満** |

### 3.3 戦略モードの不一致

| 候補 | 意図した戦略 | 実際の結果 | 評価 |
|------|------------|----------|------|
| operational_24M | - | momentum | ✅ 問題なし |
| 12M_momentum | momentum | momentum | ✅ 一致 |
| 12M_reversal | reversal | momentum | ⚠️ **不一致** |

**12M_reversalの問題:**
- Study B（reversal向け）で最適化したが、最適解がmomentum寄りになった
- これは、reversal向けの探索範囲では、実際にはmomentumの方が良い結果になったことを示唆

---

## 4. 判断が必要なポイント

### 4.1 12M_reversalの扱い

**選択肢:**

1. **現状のまま使用**
   - 過学習の可能性を認識した上で使用
   - レジーム切替で使用する場合、パフォーマンスが低い可能性がある

2. **再最適化を実行**
   - Study Bの探索範囲を調整（reversal寄りに強制）
   - 試行回数を増やす（200 → 300-500）
   - 正則化を追加（過学習対策）

3. **12M_reversalを削除**
   - `12M_momentum`のみを使用
   - レジーム切替では`12M_momentum`と`operational_24M`のみを使用

### 4.2 最適化プロセスの妥当性

**確認事項:**
- ✅ データリーク対策は適切に実装されている
- ✅ 時系列分割は正しく行われている
- ✅ ホライズン未達の期間は除外されている
- ⚠️ 12M_reversalで過学習が発生している可能性

**質問:**
1. 12M_reversalの過学習は、最適化プロセスの問題か、それともデータ/市場環境の問題か？
2. Study B（reversal向け）で最適化したのにmomentum寄りになったのは、探索範囲の問題か、それとも実際にmomentumの方が良いのか？
3. 勝率45.83%は、reversal戦略として許容できる範囲か？

### 4.3 次のステップ

**推奨される順序:**

1. **Step 2: A-1比較の再集計**
   - 共通のrebalance_date集合で比較
   - 24Mを含める比較と12Mだけの比較の2本を出力
   - 現状の候補で実行可能

2. **12M_reversalの再検討**
   - ChatGPTの判断を仰いだ後、再最適化するか決定
   - 再最適化する場合、探索範囲や試行回数を調整

3. **Step 3: レジームポリシーの見直し**
   - 候補が確定した後、レジーム切替ポリシーを最適化

---

## 5. 技術的な詳細

### 5.1 戦略モード判定ロジック（修正後）

```python
def determine_strategy_mode(best_params):
    rsi_base = best_params["rsi_base"]
    rsi_max = best_params["rsi_max"]
    bb_z_base = best_params["bb_z_base"]
    bb_z_max = best_params["bb_z_max"]
    bb_weight = best_params["bb_weight"]
    rsi_weight = 1.0 - bb_weight
    
    # RSIの方向: 順張り=+1, 逆張り=-1
    rsi_direction = 1.0 if rsi_max > rsi_base else -1.0
    
    # BBの方向: 順張り=+1, 逆張り=-1
    bb_direction = 1.0 if bb_z_max > bb_z_base else -1.0
    
    # 重み付けスコア
    weighted_score = (rsi_direction * rsi_weight) + (bb_direction * bb_weight)
    
    return "momentum" if weighted_score > 0 else "reversal"
```

### 5.2 Study Typeの設定

- **Study A**: BB寄り・低ROE閾値（momentum向け）
  - `w_value`: 0.10-0.50
  - `w_forward_per`: 0.20-0.90
  - `roe_min`: 0.00-0.12
  - `bb_weight`: 0.30-0.95

- **Study B**: Value寄り・ROE閾値やや高め（reversal向け）
  - `w_value`: 0.20-0.60
  - `w_forward_per`: 0.20-0.80
  - `roe_min`: 0.00-0.20
  - `bb_weight`: 0.20-0.80

- **Study C**: Study A/B統合・広範囲探索
  - 上記の統合範囲

### 5.3 データ分割の詳細

- **学習期間**: 2020-01-01 ～ 2022-12-31（train_end_date="2022-12-31"）
- **テスト期間**: 2023-01-01 ～ 2025-12-31
- **分割方法**: 時系列分割（`split_rebalance_dates` with `train_end_date`）
- **リバランス日**: 毎月末日

---

## 6. ChatGPTへの質問

1. **12M_reversalの過学習について**
   - 学習12.86% vs テスト1.36%（差+11.50%）は、最適化プロセスの問題か、それともデータ/市場環境の問題か？
   - 勝率45.83%（50%未満）は、reversal戦略として許容できる範囲か？

2. **戦略モードの不一致について**
   - Study B（reversal向け）で最適化したのに、結果がmomentum寄りになったのは、探索範囲の問題か、それとも実際にmomentumの方が良いのか？
   - この場合、再最適化すべきか、それとも現状の結果を受け入れるべきか？

3. **次のステップについて**
   - 12M_reversalを再最適化すべきか、それとも現状のままStep 2（A-1比較）に進むべきか？
   - 再最適化する場合、どのような調整が推奨されるか？（探索範囲、試行回数、正則化など）

4. **最適化プロセスの妥当性について**
   - データリーク対策、時系列分割、ホライズン未達の除外など、最適化プロセスに問題はないか？
   - 12M_reversalの過学習は、プロセスの問題か、それともデータ/市場環境の問題か？

---

## 7. 補足情報

### 7.1 最適化の実行ログ

- **operational_24M**: 完了trial数: 310/200, 総試行数: 318, pruned: 0
  - **注意**: 既存study（`operational_24M_20260109`）を再利用したため、累積で310 trialとなっている。新規の最適化試行は200 trial。
- **12M_momentum**: 正常に完了（200試行）
- **12M_reversal**: 正常に完了（200試行、ただし、過学習の可能性）

### 7.2 保存されたファイル

- `optimization_result_operational_24M_20260109.json`
- `params_operational_24M_20260109.json`
- `optimization_result_12M_momentum_20260109.json`
- `params_12M_momentum_20260109.json`
- `optimization_result_12M_reversal_20260109.json`
- `params_12M_reversal_20260109.json`
- `config/params_registry_longterm.json`（更新済み）

### 7.3 修正済みの点

- ✅ 戦略モード判定ロジックを改善（RSIとBBの両方を考慮）
- ✅ `12M_momentum`の戦略モードを修正（reversal → momentum）
- ✅ レジストリを更新

---

---

## 8. ChatGPTからのフィードバックと推奨事項

### 8.1 12M_reversalの「過学習」の原因分析

**結論**: 「純粋な実装リーク」よりも、**(A) 非定常（市場環境差）＋(B) 目的関数と探索空間の相性で"学習期間に偶然フィット"した**可能性が濃厚。

#### A. 市場環境の変化（非定常）

- train: 2020-01-01〜2022-12-31、test: 2023-01-01〜2025-12-31 という分割は、構造変化が起きやすい区切り
- 12M_reversalは**平均はプラス（テスト平均 1.36%）なのに中央値がマイナス（-1.51%）**で、勝率も 45.83% と低い
- **「勝つときだけ大きく勝つ」形ですらなく、分布として弱い**（＝単に効いていない）可能性が高い

#### B. 目的関数が「平均」中心で、外れ値に弱い

- 最適化評価が「年率超過リターン（平均）」中心
- 平均は少数の大きな勝ち（月）があると簡単に押し上がる
- Optuna(TPE)が**"trainで平均だけを押し上げる尖ったパラメータ"**を拾いやすい

**見分け方（追加分析が必要）:**
- train側の**中央値・分位点（P25/P75）**、および**月別リターンの分布**を出す
- train 12.86% が "数ヶ月の特大勝ち"に依存していないか確認
  → 依存していれば、典型的な「平均最適化の過学習」

#### C. 「reversal向けStudy B」でも、最適解が momentum 側に逃げている

- 12M_reversalは、Study B（reversal向け）で最適化したのに、結果は判定ロジック上 momentum
- これは次のどちらか：
  1. **その探索空間の中で、reversalとして勝てる領域がそもそも弱い**（＝データ上、reversalが優位でない）
  2. **reversalを意図した制約が弱く、momentum的な領域も普通に入っている**（＝「Study B＝reversal」になっていない）

### 8.2 「戦略モードの不一致」の扱い

**結論**: **"不一致＝バグ"とは限らない**。ただし運用・レジーム切替で使うなら、**名前と実体がズレたまま残すのは危険**。

**おすすめの扱い（3択）:**

#### (A) 受け入れてリネーム（mode判定を正として扱う）

- 12M_reversal は mode判定上 momentum なので、**候補名を "12M_reversal" のまま残さない**
- ただし、test平均 1.36%、勝率 45.83%、中央値 -1.51% なので、**momentumとしても採用しにくい**
  → "リネームして残す"より、"消す"か"作り直す"が現実的

#### (B) "reversal専用"を本当に作る（制約を強める）

「レジーム切替にreversal枠が必要」という設計思想が強いなら、次のように**探索空間側でreversalを強制**した方が良い：

- 例：
  - `rsi_max < rsi_base` を必須（RSIは逆張り方向固定）
  - `bb_z_max < bb_z_base` を必須（BBも逆張り方向固定）
  - あるいは `determine_strategy_mode(best_params)` が reversal になるものだけ採用（制約違反は棄却/ペナルティ）
- これで「Study Bなのにmomentumになってしまう」を構造的に防げる
- ただし**reversalを強制した結果、testでさらに悪化する**なら、データ上そのreversal枠は要らない（またはレジーム条件を切り直す）という判断ができる

#### (C) "reversal枠"を一旦やめる（候補削除）

今の12M_reversalは test の統計が厳しいので、**運用候補としては外す**のが安全。
レジーム切替は**`12M_momentum` と `operational_24M` の2候補**で成立。

### 8.3 再最適化は必要か？

**結論**: **12M_reversal だけは「再最適化 or 削除」判断が必要**。他2つは「追加検証で十分」。

#### operational_24M

- testが train を上回り、良い兆候。ただし**testサンプルが12（=2023年の12回）**で小さい
- 12/12勝率は見栄えが良いが、統計的にはまだ強くない（参考：12/12の勝率でも95%下限はだいたい0.74程度）

**提案**: 再最適化より先に
- 同一条件で**seed違いの再実行**（Optuna sampler seed固定/変更）
- 指標の**中央値・分位点**も出す（平均だけだと判断がぶれる）
  を優先。

#### 12M_momentum

- train/test差が小さく、勝率も 24/36
- 当面は再最適化不要寄り

#### 12M_reversal

- train/test差が大きく、勝率も低い。加えて「reversalとして成立していない」
  → **再最適化するなら "reversalを強制する設計で"**、しないなら削除が妥当

### 8.4 最適化プロセスの妥当性

資料に書かれているリーク対策は、方向としてかなり良い。特に：

- `require_full_horizon=True`
- `as_of_date` 明示（DB maxを使わない）
- 時系列分割（train_end_date）
- `WHERE date <= as_of_date` で価格の物理切り取り
- 財務データの `disclosed_date <= rebalance_date` フィルタ

このセットは"典型リーク"を相当防げている。

#### ただし、1点だけ "厳密OOS" の観点で補足推奨

**train/testを rebalance_date で切っている場合、ラベル（12M/24Mリターン）の期間がtest側に食い込む**ことがある。
これは「未来参照リーク」ではないが、**検証の独立性が少し落ちる**（近いサンプル間で同じ価格変動を共有する）ので、より厳密にするなら：

- trainに入れるのは `eval_end_date <= train_end_date` を満たすrebalanceだけ
  （12Mなら train_end_date が 2022-12-31 の場合、rebalanceは 2021-12-31 まで）
- もしくは purge/embargo（ホライズン分ギャップ）を入れる

この補足を入れると、「過学習か？市場環境差か？」の切り分けがしやすくなる。

### 8.5 推奨アクション（短期の意思決定順）

1. **Step 2（A-1比較の再集計）を先に実施**（資料にもある通り）
2. その上で
   - 12M_reversal は **削除 or "reversal強制"で再最適化**
   - 12M_momentum と operational_24M は **採用候補として追加検証（分位点・seed耐性）**
3. 候補が固まったらレジームポリシー最適化へ

### 8.6 追加分析が必要な項目

**12M_reversalが"過学習"なのか"非定常"なのか**を高精度で判定するために、以下が必要：

1. **train期間の超過リターンの中央値・P25/P75**（12M_reversalだけでもOK）
2. **testを2023/2024/2025に分けた年別成績**（12M_reversalだけでもOK）

この2つを資料に追記できれば、「再最適化するならどういう制約・目的関数が効くか」まで、より具体に提案できる。

### 8.7 資料への追加・修正推奨

#### 追加推奨

1. **train側の中央値・分位点（P25/P75）**
   → 12M_reversal が「平均だけ高い」のか見抜ける

2. **年別（2020/21/22、2023/24/25） or レジーム別の成績**
   → レジーム切替をする設計なら、候補もレジーム別に強みがあるか確認したい

3. **24Mの"サンプル12問題"の注記と、補助評価案**
   24Mはどうしても cohort数が少ないので、補助として
   - calendar-time（重なりポートフォリオの月次系列化）
   の評価を併記すると説得力が上がる（24Mでも月次系列が取れるようになる）

4. **Optunaの再現性情報**
   sampler seed、同条件での複数回結果（平均・分散）
   → "一発当たり"を排除できる

#### 修正推奨（軽微）

- operational_24M の「完了trial数: 310/200」表記は、読者が混乱しやすいので
  「既存studyを再利用したため累積で310 trial」など理由を書いておくと良い

---

**作成日**: 2026-01-09  
**最終更新**: 2026-01-09（ChatGPTフィードバック反映）  
**作成者**: AI Assistant  
**目的**: ChatGPTへの判断依頼用資料

