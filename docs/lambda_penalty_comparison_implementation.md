# 下振れ罰係数λの比較実装資料（ChatGPT検証用）

## 1. 実装の背景と目的

### 背景
固定ホライズン最適化において、目的関数が「平均超過リターン」のみを最大化する場合、以下の問題が発生する可能性があります：

- **外れ値への過敏性**: 少数の大きな勝ち（月）があると平均が簡単に押し上がる
- **下振れリスクの無視**: 平均は高いが、P10（下位10%）や最小値が非常に低いパラメータが選ばれやすい
- **過学習リスク**: train期間で平均だけを押し上げる「尖ったパラメータ」を選びやすい

### 目的
下振れ罰係数λを導入することで、平均超過だけでなく、下振れリスク（P10超過リターン）も考慮した最適化を実現します。

**目的関数**:
```
objective = mean_annual_excess_return_pct + λ × min(0, p10_annual_excess_return_pct)
```

**注**: `min(0, p10_excess)`は常に0以下なので、実質的には減算されます。
- λ=0.00: 下振れ罰なし（現状の平均最大化）
- λ>0.00: P10が負の場合にペナルティ（係数λで調整）

**より直感的な表記（等価）**:
```
penalty = λ × max(0, -p10_annual_excess_return_pct)
objective = mean_annual_excess_return_pct - penalty
```

### 検証したい仮説
複数のλ値（0.00, 0.05, 0.03, 0.08）で最適化を実行し、以下を比較検証：
1. **平均超過リターン**: λが大きくなるほど平均が下がるか？
2. **P10（下位10%）**: λが大きくなるほど下振れが改善するか？
3. **勝率**: λが大きくなるほど勝率が上がるか？
4. **最適なλ値**: 平均と下振れのバランスが取れるλは？

---

## 2. 実装内容の詳細

### 2.1 目的関数の修正（`optimize_longterm.py`）

#### 修正箇所
- `objective_longterm`関数に`lambda_penalty: float = 0.0`パラメータを追加
- 目的関数の計算式を変更

#### 修正前
```python
# 平均超過のみ
objective_value = mean_excess
```

#### 修正後
```python
# 平均超過 - 下振れ罰
mean_excess = perf["mean_annual_excess_return_pct"]
p10_excess = perf.get("p10_annual_excess_return_pct", 0.0)
downside_penalty = lambda_penalty * min(0.0, p10_excess)  # P10が負の場合のみペナルティ
objective_value = mean_excess + downside_penalty  # downside_penaltyは負の値なので実質的に減算
```

#### 重要な設計判断
- **P10が正の場合**: ペナルティは0（`min(0.0, p10_excess)`により）
- **P10が負の場合**: 係数λでペナルティ（例: λ=0.05なら、P10=-5%で`0.05 × (-5%) = -0.25%`のペナルティ）
- **「+ downside_penalty」の理由**: `downside_penalty`は負の値なので、実質的に減算される
  - 例: `mean_excess=3.5%`, `downside_penalty=-0.25%` → `objective=3.5% + (-0.25%) = 3.25%`

#### P10の定義と計算方法
- **定義**: 各リバランス日のポートフォリオの年率超過リターンの分布から、10パーセンタイル値を取得
- **計算方法**: `numpy.percentile(annual_excess_returns, 10.0)`（補間方式: linear）
- **単位**: **%ポイント**（例: 3.5 = 3.5%、-5.2 = -5.2%）
- **サンプル数が少ない場合**: サンプルが1件の場合はP10=その値、0件の場合は0.0を返す
- **欠損時の扱い**: `perf.get("p10_annual_excess_return_pct", 0.0)`により、欠損時は0.0となる
  - **注意**: 欠損時（P10が算出不能）でも罰がゼロになるため、サンプル数が少ない場合（特に24Mでtrainが短い/少ない）はP10が不安定になる可能性がある
  - **推奨改善案（ChatGPT推奨）**: 
    * `n_periods`（P10算出に使ったサンプル数）も`perf`に入れて出力（train/testそれぞれで必ず表に載せる）
    * `n_periods < N_min`（例：12Mなら12、24Mなら10等）ならtrial無効（`-inf`）または`p10_excess`を最悪側に寄せる（例：min値や強い罰）
    * 24Mはtestサンプル数が少ないので、P10ではなく**CVaR10（下位10%平均）**や**P20**を推奨（資料の将来案ではなく、24Mでは推奨として明記）

#### 平均超過リターンの単位
- **単位**: **%ポイント**（例: 3.5 = 3.5%、1.33 = 1.33%）
- **注意**: `*_pct`という命名規則により、コード内では%ポイントで統一されている
- **目的関数の単位**: `mean_excess`も`p10_excess`も`downside_penalty`もすべて%ポイントで統一されている

### 2.2 最適化関数へのパラメータ追加（`optimize_longterm.py`）

#### 修正箇所
- `main`関数に`lambda_penalty: float = 0.0`パラメータを追加
- `objective_longterm`への呼び出し時に`lambda_penalty`を渡す
- コマンドライン引数に`--lambda-penalty`を追加

#### コマンドライン引数
```bash
python -m omanta_3rd.jobs.optimize_longterm \
  --lambda-penalty 0.05 \
  ...
```

### 2.3 再最適化スクリプトへの反映（`reoptimize_all_candidates.py`）

#### 修正箇所
- `optimize_and_save`関数に`lambda_penalty: float = 0.0`パラメータを追加
- `main`関数に`lambda_penalty`パラメータを追加
- コマンドライン引数に`--lambda-penalty`を追加
- 各候補の最適化時に`lambda_penalty`を渡す

#### 使用例
```bash
python -m omanta_3rd.jobs.reoptimize_all_candidates \
  --lambda-penalty 0.05 \
  ...
```

### 2.4 複数λ値の比較スクリプト（`compare_lambda_penalties.py`）

#### 新規作成
複数のλ値で最適化を並走実行し、結果を比較するスクリプトを作成しました。

#### 主な機能
1. **複数λ値での並走最適化**: デフォルトでλ=0.00, 0.05, 0.03, 0.08の4本を実行
2. **同じ条件での実行**: 固定ホライズン、同じtrain/test分割で実行（公平比較）
3. **test期間での評価**: 最適化後にtest期間（train_end_dateより後）でバックテストを実行
4. **比較指標の出力**: 平均超過、P10、勝率、切替回数を比較

#### 処理フロー
```
for each lambda_value in [0.00, 0.05, 0.03, 0.08]:
    1. optimize_longterm_main(lambda_penalty=lambda_value) を実行
       - train期間で最適化
       - test期間で評価（optimize_longterm_main内）
    2. 最適化結果JSONを読み込む
    3. パラメータファイルを保存（save_params_file）
    4. test期間で追加バックテストを実行（run_backtest_with_params_file）
       - 固定パラメータモードでバックテスト
       - eval_end = rebalance_date + horizon_months（固定ホライズン）
    5. 結果を集計（平均超過、P10、勝率、切替回数）
```

#### 重要な設計判断

**1. test期間の定義**
- train期間: `start_date` ～ `train_end_date`（デフォルト: 2022-12-31）
- test期間: `train_end_date`より後のリバランス日 ～ `end_date`

**2. 固定パラメータモードでのバックテスト**
- `run_monthly_portfolio_with_regime`を`fixed_params_id`で呼び出し
- レジーム切替は行わないため、切替回数は常に0
- **注意**: レジーム切替戦略での切替回数が必要な場合は別途実装が必要

**3. 固定ホライズン評価**
- 各ポートフォリオは`rebalance_date + horizon_months`まで評価
- `require_full_horizon=True`で、満了窓未達の期間を除外

**4. 24Mホライズンの調整**
- 24Mの場合は`rebalance_end_date`を24か月前倒し
- 例: `end_date=2025-12-31`の場合、`rebalance_end_date=2023-12-31`
- これにより、24Mが完走できる期間で最適化

---

### 2.5 固定ホライズン評価とtrain/test分割の実装確認

#### 固定ホライズン評価の実装（コードベース確認済み）

**実装箇所**（`optimize_longterm.py` 行320-357）:
```python
# eval_end_dateを計算（固定ホライズン）
eval_end_dt = rebalance_dt + relativedelta(months=horizon_months)
eval_end_date = eval_end_dt.strftime("%Y-%m-%d")

# require_full_horizonがTrueの場合、eval_end_dateがas_of_dateより後の場合は除外
if require_full_horizon:
    if eval_end_dt > as_of_dt:
        continue  # 満了窓未達のため除外

# eval_end_dateとas_of_dateのうち、早い方を使用（安全のため）
# 注意: require_full_horizon=Trueなら実質eval_end_date（as_of_date >= eval_end_dateが保証されている）
# 重要: 最適化・比較のときはrequire_full_horizon=Trueを強制（固定ホライズン評価を保証するため）
eval_date = min(eval_end_date, as_of_date)
# アサーション: require_full_horizon=Trueならeval_date==eval_end_dateであるべき
assert not require_full_horizon or eval_date == eval_end_date, f"require_full_horizon=True but eval_date({eval_date}) != eval_end_date({eval_end_date})"

# eval_dateを営業日にスナップ（非営業日の場合は前営業日を使用）
# 重要: スナップ関数の引数はeval_date（=eval_end_date）である必要がある（固定ホライズンを守るため）
eval_date_snapped = _snap_price_date(conn, eval_date)

# パフォーマンスを計算（固定ホライズンで評価）
perf = calculate_portfolio_performance(rebalance_date, eval_date_snapped)
```

**固定ホライズン評価の証拠ログ（実装例）**（行350）:
```
[calculate_longterm_performance] 2020-01-31 → eval_end=2022-01-31 (holding=2.00年, horizon=24M)
[calculate_longterm_performance] 2020-02-28 → eval_end=2022-02-28 (holding=2.00年, horizon=24M)
```

**検証ポイント**:
- `rebalance_date`: 2020-01-31
- `eval_end_date`（計算値）: 2020-01-31 + 24M = 2022-01-31
- `performance_end_date`（実際に評価関数に渡した値）: 2022-01-31（`eval_date_snapped`）
- **確認**: `performance_end_date == eval_end_date` ✅（固定ホライズン評価が成立）

#### 非営業日の丸め規約

**規約**: `eval_date`が非営業日の場合は、その日以前の最新の営業日を使用（`_snap_price_date`関数）

**実装**（`optimize_longterm.py` 行336-346）:
```python
# eval_dateを営業日にスナップ（非営業日の場合は前営業日を使用）
# 規約: eval_dateが非営業日の場合は、その日以前の最新の営業日を使用
# 重要: スナップ関数の引数はeval_date（=eval_end_date）である必要がある（固定ホライズンを守るため）
eval_date_snapped = _snap_price_date(conn, eval_date)
if eval_date_snapped != eval_date:
    print(f"      [calculate_longterm_performance] eval_dateを営業日にスナップ: {eval_date} → {eval_date_snapped}")
```

**例**: `eval_end_date = 2022-01-30`（日曜日）の場合、`eval_date_snapped = 2022-01-28`（金曜日）となる

**注意点（ChatGPT推奨）**:
- **データ欠損でスナップが大きく戻るケース**: `eval_end`近辺の価格が欠損していると、スナップが数日ではなく数週間〜数ヶ月戻り得ます
- **対処法**: ログに`eval_end`、`eval_date_snapped`、差分（日数）を出力し、差分が大きいもの（例: 7日以上）は除外するのが堅い
  - 実装例（推奨追加）:
    ```python
    snap_diff_days = (datetime.strptime(eval_date, "%Y-%m-%d") - datetime.strptime(eval_date_snapped, "%Y-%m-%d")).days
    if snap_diff_days > 7:  # 1週間以上のズレは除外
        print(f"      [calculate_longterm_performance] ⚠️  {rebalance_date}のeval_dateスナップ差分が大きい（{snap_diff_days}日）のため除外")
        continue
    ```

#### 価格データの物理フィルタ（未来参照防止）

**価格データの取得**（`_snap_price_date`関数内）:
```sql
SELECT MAX(date) AS d FROM prices_daily WHERE date <= ?
```
- パラメータ: `as_of_date`（評価の打ち切り日）
- **重要**: `date <= as_of_date` で物理的にフィルタしているため、未来参照を防止

**バックテスト内の価格取得**（`calculate_portfolio_performance`）:
- 全ての価格データ取得は `date <= eval_date`（`eval_date <= as_of_date`）でフィルタされる
- これにより、未来参照を完全に防止

### リバランス開始日の考慮（過去3年のデータ要件）

**要件**: 財務データの計算（営業利益トレンドなど）には過去3年のデータが必要（`_load_fy_history(conn, price_date, years=3)`）

**実装確認**:
- `build_features`関数内で`fy_hist = _load_fy_history(conn, price_date, years=3)`を呼び出し
- `op_trend`の計算で`tail(3)`を使用（過去3年分の営業利益データが必要）

**リバランス開始日の判断**:
- データが2015年からある場合、2017-2018年からリバランス可能
  - 理由: 2015年3月決算のデータが2015年5月に開示される場合、2017年5月時点で過去3年分（2015、2016、2017年度）のデータが揃う
  - より安全な目安: 2018年1月以降（2017年度決算が開示される時期）

**現在の実装の注意点**:
- 過去3年分のデータが揃っているかのチェックは実装されていない
- `_load_fy_history`が返すデータが3年分揃っているかどうかは確認されていない
- データが不足している場合でも、エラーにならずに計算が続行される（結果として`op_trend`がNaNになる可能性がある）

**将来の改善案**:
- 過去3年分のデータが揃っているかをチェックし、不足している場合はリバランスをスキップする機能を追加
- `start_date`を指定する際の推奨値として、「データ開始年 + 3年」を明記
  - 例: データが2015年からある場合、`start_date = "2018-01-01"`を推奨

#### train/test分割の実装（厳密な分割）

**実装箇所**（`optimize_longterm.py` 行110-126）:
```python
# 固定ホライズン制約を適用（train期間では`eval_end <= train_end_date`を満たすものだけを使用）
if train_end_date is not None:
    train_dates = []
    train_end_dt = datetime.strptime(train_end_date, "%Y-%m-%d")
    for rb_date in all_dates:
        rb_dt = datetime.strptime(rb_date, "%Y-%m-%d")
        eval_end_dt = rb_dt + relativedelta(months=horizon_months)
        # eval_end <= train_end_date を満たすものだけをtrainに含める
        if eval_end_dt <= train_end_dt:
            train_dates.append(rb_date)
```

**実装例**:
- **12Mの場合**: `train_end_date = 2022-12-31` なら、train rb の上限は `2022-12-31 - 12M = 2021-12-31` 以前
- **24Mの場合**: `train_end_date = 2022-12-31` なら、train rb の上限は `2022-12-31 - 24M = 2020-12-31` 以前

**なぜこの分割が必要か**:
固定ホライズン（12M/24M）では、1サンプルが **rbからhorizon先までのリターン**で評価されます。
もし trainに `rb=2022-12` を入れた場合（24Mなら）:
- `eval_end = 2024-12`
- **trainのラベル作成に test期間（2023〜2024）の価格が混ざる**可能性がある

**厳密な分割の効果**:
- train期間では `eval_end <= train_end_date` を満たすものだけを使用
- これにより、**train期間のラベル作成にtest期間の価格が混ざらない**ことを保証
- train/testの独立性を維持し、過学習検知が正確になる

**24Mの場合のサンプル数への影響**:
- **注意**: 24Mは厳密な分割だとtrainサンプルが減りやすい
- 例: `start_date = 2020-01-01`, `train_end_date = 2022-12-31` の場合
  - 24Mの場合: train rb の上限は `2020-12-31` 以前 → **trainサンプルが大幅に減少**
  - 12Mの場合: train rb の上限は `2021-12-31` 以前 → trainサンプルは比較的多い
- **対処法**: 24Mの場合は `start_date` をもっと過去に伸ばす（例: 2018-01-01）ことで、trainサンプル数を確保

---

## 3. 使用方法

### 3.1 単一λ値での最適化

```bash
# λ=0.05で最適化
python -m omanta_3rd.jobs.optimize_longterm \
  --start 2020-01-01 \
  --end 2025-12-31 \
  --study-type C \
  --n-trials 200 \
  --lambda-penalty 0.05 \
  --horizon-months 24 \
  --as-of-date 2025-12-31 \
  --train-end-date 2022-12-31
```

### 3.2 複数λ値での比較

```bash
# デフォルトの4本（0.00, 0.05, 0.03, 0.08）を並走
python -m omanta_3rd.jobs.compare_lambda_penalties \
  --params-id operational_24M \
  --start 2020-01-01 \
  --end 2025-12-31 \
  --n-trials 200 \
  --train-end-date 2022-12-31 \
  --as-of-date 2025-12-31

# カスタムλ値で比較
python -m omanta_3rd.jobs.compare_lambda_penalties \
  --params-id 12M_momentum \
  --start 2020-01-01 \
  --end 2025-12-31 \
  --n-trials 200 \
  --lambda-values 0.00 0.03 0.05 0.08 0.10 \
  --train-end-date 2022-12-31
```

### 3.3 再最適化スクリプトでの使用

```bash
# すべての候補をλ=0.05で再最適化
python -m omanta_3rd.jobs.reoptimize_all_candidates \
  --start 2020-01-01 \
  --end 2025-12-31 \
  --n-trials 200 \
  --lambda-penalty 0.05 \
  --train-end-date 2022-12-31
```

---

## 4. 出力形式と結果の解釈

### 4.1 比較結果の出力（Markdown形式）

```markdown
| λ値 | 平均超過(%) | P10(超過)(%) | 勝率(%) | 切替回数 | 期間数 | train超過(%) | test超過(%) |
|-----|------------|-------------|---------|---------|--------|-------------|-------------|
| 0.00 | 3.50 | -5.20 | 55.0 | 0 | 24 | 4.20 | 3.50 |
| 0.03 | 3.45 | -4.80 | 58.3 | 0 | 24 | 4.15 | 3.45 |
| 0.05 | 3.40 | -4.50 | 62.5 | 0 | 24 | 4.10 | 3.40 |
| 0.08 | 3.30 | -4.20 | 66.7 | 0 | 24 | 4.00 | 3.30 |
```

### 4.2 結果の解釈ポイント

#### A. 平均超過 vs P10のトレードオフ
- **λ=0.00**: 平均は高いが、P10が低い（下振れリスク大）
- **λ=0.05**: 平均は少し下がるが、P10が改善（下振れリスク減）
- **λ=0.08**: 平均はさらに下がるが、P10がさらに改善

#### B. train/testのギャップ
- **train超過 > test超過**: 過学習の可能性（特にλ=0.00）
- **train超過 ≈ test超過**: 汎化性能が良い（λが大きいほど改善する可能性）

#### C. 勝率の推移
- **λが大きくなるほど勝率が上がる**: 下振れを避ける方向に最適化されている
- **勝率が上がらない**: 下振れ罰が効いていない可能性

### 4.3 最適なλ値の選定基準

#### 期待される傾向（一般的に期待されるが、必ずしも単調増加/改善ではない）

最適化はノイズもあるため、以下の傾向は「期待される」ものであり、必ずしも単調増加/改善とは限りません。

1. **P10の改善傾向**: λが大きくなるほどP10が改善する傾向がある（下振れリスクが減る）
2. **平均超過の減少傾向**: λが大きくなるほど平均が下がる傾向がある（トレードオフ）
3. **勝率の向上傾向**: λが大きくなるほど勝率が上がる傾向がある（安定性の向上）
4. **train/testギャップの縮小傾向**: λが大きくなるほどtrain/testギャップが縮小する傾向がある（汎化性能の改善）

#### 実務でブレにくい選び方（推奨）

**おすすめA：最小λルール（保守的で安定）**

`λ=0` に比べて以下を満たす **最小のλ**を採用:
- P10が一定以上改善（例：+0.5〜+1.0%）
- 平均の低下が許容範囲（例：-0.2〜-0.3%以内）

→ λを大きくしすぎて別の過学習をするリスクが減ります

**おすすめB：valで同じ目的関数を最大化（最も筋が良い）**

λは"ハイパーパラメータ"なので:
1. trainで最適化（各λ値でパラメータを最適化）
2. val（testの一部）で `mean + λ*min(0,p10)` を最大化するλを選ぶ
3. 最後に holdout（完全未使用、例：2025年だけ）で確認

**推奨分割例（ChatGPT推奨）**:
- train: 2020-01-01 ～ 2022-12-31（最適化用）
- val: 2023-01-01 ～ 2024-12-31（λ選定用）
- holdout: 2025-01-01 ～ 2025-12-31（最終確認用、完全未使用）

→ これが一番筋が良いです（現在の資料はtrain/testの2分割なので、最終確認用holdoutを明記すると完璧）

#### P10の安定性に関する注意（ChatGPT推奨）

**問題**: P10はサンプル数が少ないとブレます。特に24Mはrb数が減るので、P10はほぼ「最悪値」に近づきがちです。

**24Mでは推奨（ChatGPT推奨）**:
- 24Mはtestサンプル数が少ない（12本程度）ので、**P10だけでλを決めない**
- **CVaR10（下位10%平均）**または**P20**を推奨（資料の将来案ではなく、24Mでは推奨として明記）
- これらにより、サンプル数が少ない場合でも安定した指標を得られる

**12Mの場合**:
- サンプル数が比較的多い（24-36本程度）ので、P10でも比較的安定
- ただし、`n_periods`を必ず出力して、サンプル数が少ないケースを検知する

**最適化のノイズ対策（ChatGPT推奨）**:
- λ比較は "目的関数が違う＝別問題" なので、Optunaのブレが出やすい
- **推奨**: sampler seedを変えて2回（できれば3回）回し、平均/分散を見る
  - 実装例: `--random-seed 42`, `--random-seed 100`, `--random-seed 200`で3回実行
  - 最適化結果（best_value、best_params）の平均・分散を比較
  - これにより、λ選定がかなり信頼できる

#### 例: 判断シナリオ（単位: %ポイント）

```
シナリオ1: λ=0.05が最適
- λ=0.00: 平均3.50%、P10=-5.20%、勝率55%
- λ=0.05: 平均3.40%（-0.10%ポイント）、P10=-4.50%（+0.70%ポイント）、勝率62.5%（+7.5%ポイント）
→ 平均の減少が小さく、P10と勝率が大きく改善

シナリオ2: λ=0.03が最適
- λ=0.00: 平均3.50%、P10=-5.20%、勝率55%
- λ=0.05: 平均3.25%（-0.25%ポイント）、P10=-4.80%（+0.40%ポイント）、勝率58%（+3%ポイント）
→ 平均の減少が大きいため、λ=0.03を選択（バランス重視）
```

**注意**: 上記の「+0.70%ポイント」は**絶対値**での改善を意味します（相対%ではない）

---

## 5. 検証すべきポイント

### 5.1 実装の正確性

#### A. 目的関数の計算が正しいか
- [x] `downside_penalty = lambda_penalty * min(0.0, p10_excess)`が正しく計算されているか ✅（確認済み）
- [x] P10が正の場合、ペナルティが0になるか ✅（`min(0.0, p10_excess)`により）
- [x] P10が負の場合、適切なペナルティがかかるか ✅（例: λ=0.05, P10=-5% → `0.05 × (-5%) = -0.25%`のペナルティ）
- [ ] **P10欠損時の扱い**: `perf.get("p10_annual_excess_return_pct", 0.0)`により、欠損時は0.0になる → これは想定通りか、または改善が必要か？

#### B. 固定ホライズン評価が正しいか
- [x] test期間でのバックテストが`eval_end = rebalance_date + horizon_months`で実行されているか ✅（確認済み、行320-357）
- [x] `require_full_horizon=True`で満了窓未達の期間が除外されているか ✅（確認済み、行324-330）
- [x] 24Mの場合は`rebalance_end_date`が正しく調整されているか ✅（確認済み、`compare_lambda_penalties.py`行304-307）
- [x] **固定ホライズン評価の証拠ログ**: `performance_end_date == eval_end_date` を確認 ✅（行350でログ出力）

#### C. train/test分割が正しいか
- [x] train期間（～train_end_date）で最適化されているか ✅（確認済み、行110-126）
- [x] test期間（train_end_dateより後）で評価されているか ✅（確認済み）
- [x] train期間では`eval_end <= train_end_date`を満たすものだけを使用 ✅（確認済み、行117）
- [x] train期間とtest期間が重複していないか ✅（厳密な分割により保証）

#### D. リバランス開始日の考慮
- [x] 過去3年のデータ要件が考慮されているか ✅（確認済み、`_load_fy_history(conn, price_date, years=3)`）
- [ ] **過去3年分のデータが揃っているかのチェック**: 現在は実装されていない（将来の改善案として提案）
  - **推奨**: `start_date`を指定する際は「データ開始年 + 3年」を推奨（例: 2015年からデータがある場合、`start_date = "2018-01-01"`）

### 5.2 結果の妥当性

#### A. λ値の効果が現れているか（期待される傾向）
- [ ] λが大きくなるほど平均超過が下がる傾向があるか（一般的に期待される）
  - **注意**: 必ずしも単調減少とは限らない（ノイズの影響）
- [ ] λが大きくなるほどP10が改善する傾向があるか（期待される効果）
  - **注意**: 必ずしも単調増加とは限らない（ノイズの影響）
- [ ] λが大きくなるほど勝率が上がる傾向があるか（期待される効果）
  - **注意**: 必ずしも単調増加とは限らない（ノイズの影響）

#### B. train/testギャップの分析
- [ ] train超過とtest超過のギャップが適切か（過学習の兆候がないか）
  - **基準**: train/testギャップが大きい（例: >1%）場合、過学習の可能性
- [ ] λが大きくなるほどtrain/testギャップが縮小する傾向があるか（汎化性能の改善）
  - **注意**: 必ずしも単調縮小とは限らない（ノイズの影響）

#### C. 最適化の収束性
- [ ] 各λ値で最適化が正常に収束しているか（n_trials回完了しているか）
  - **確認方法**: 最適化ログで「完了trial数: X/n_trials」を確認
- [ ] 最適化結果が安定しているか（seed違いで再実行しても類似結果か）
  - **確認方法**: 複数回実行して、最適パラメータの差異を確認
  - **推奨（ChatGPT推奨）**: λ比較は "目的関数が違う＝別問題" なので、Optunaのブレが出る
    * **sampler seed を変えて2回（できれば3回）回し、平均/分散を見る**を「推奨チェック」に入れると、λ選定がかなり信頼できる
    * 実装例（推奨追加）: `--random-seed`を複数値で実行し、最適化結果の平均・分散を比較

### 5.3 潜在的な問題点

#### A. 過学習のリスク
- **問題**: λを大きくしすぎると、train期間の下振れに過剰に適合する可能性
- **確認方法**: train/testギャップが大きくなる場合、過学習の可能性
- **対策**: λを段階的に増やし、train/testギャップが最小のλを選択

#### B. 最適化の安定性
- **問題**: λが変わると最適化結果が大きく変わる可能性
- **確認方法**: 複数のλ値で最適化し、パラメータの変化を確認
- **対策**: 最適化結果のパラメータを比較し、大きな変化がないことを確認

#### C. test期間での評価方法
- **問題**: `compare_lambda_penalties.py`は固定パラメータモードでバックテストしているため、レジーム切替戦略での切替回数が取得できない
- **確認方法**: 切替回数が常に0になっていることを確認（これは想定通り）
- **対策**: レジーム切替戦略での切替回数が必要な場合は、別途実装が必要

### 5.4 既知の制約と注意点

#### A. 切替回数の制約
- **制約**: 固定パラメータモードでの評価のため、切替回数は常に0
- **理由**: `run_backtest_with_params_file`が`fixed_params_id`で呼び出されているため
- **対処**: レジーム切替戦略での切替回数が必要な場合は、別途実装が必要（今回は対象外）

#### B. 24Mホライズンの制約
- **制約**: 24Mの場合は`rebalance_end_date`を24か月前倒しする必要がある
- **理由**: 24Mが完走できる期間で最適化するため
- **対処**: `compare_lambda_penalties.py`で自動調整しているが、他のスクリプトでは手動調整が必要

#### C. 最適化時間
- **制約**: 複数のλ値で並走実行するため、最適化時間が長くなる（4本で約4倍）
- **理由**: 各λ値で`n_trials`回の最適化を実行するため
- **対処**: `--n-trials`を減らすか、優先順位の高いλ値だけを実行

---

## 6. 期待される結果と判断基準

### 6.1 理想的な結果パターン

#### パターン1: λ=0.05が最適（推奨）
```
| λ値 | 平均超過(%) | P10(超過)(%) | 勝率(%) |
|-----|------------|-------------|---------|
| 0.00 | 3.50 | -5.20 | 55.0 |
| 0.05 | 3.40 | -4.50 | 62.5 | ← 最適（平均-0.10%、P10+0.70%、勝率+7.5%）
| 0.08 | 3.30 | -4.20 | 66.7 | ← 平均が大きく下がる
```

#### パターン2: λ=0.03が最適（保守的）
```
| λ値 | 平均超過(%) | P10(超過)(%) | 勝率(%) |
|-----|------------|-------------|---------|
| 0.00 | 3.50 | -5.20 | 55.0 |
| 0.03 | 3.45 | -4.80 | 58.3 | ← 最適（バランス重視）
| 0.05 | 3.40 | -4.50 | 62.5 | ← P10改善が小さい
```

#### パターン3: λ=0.00が最適（下振れ罰不要）
```
| λ値 | 平均超過(%) | P10(超過)(%) | 勝率(%) |
|-----|------------|-------------|---------|
| 0.00 | 3.50 | -5.20 | 55.0 | ← 最適（下振れ罰不要）
| 0.05 | 3.20 | -4.50 | 60.0 | ← 平均が大きく下がる
```
→ この場合は、下振れ罰が不要（または目的関数の設計を見直す必要がある）

### 6.2 判断基準（優先順位）

#### 期待される傾向（一般的に期待されるが、必ずしも単調増加/改善ではない）

最適化はノイズもあるため、以下の傾向は「期待される」ものであり、必ずしも単調増加/改善とは限りません。

1. **P10の改善傾向**: λが大きくなるほどP10が改善する傾向がある（下振れリスクが減る）
2. **train/testギャップの縮小傾向**: λが大きくなるほどtrain/testギャップが縮小する傾向がある（汎化性能の改善）
3. **平均超過の減少傾向**: λが大きくなるほど平均が下がる傾向がある（トレードオフ）

#### 実務でブレにくい選び方（推奨）

**おすすめA：最小λルール（保守的で安定）**

`λ=0` に比べて以下を満たす **最小のλ**を採用:
- P10が一定以上改善（例：**+0.5〜+1.0%ポイント**）
- 平均の低下が許容範囲（例：**-0.2〜-0.3%ポイント以内**）

**注意（ChatGPT推奨）**: 上記のしきい値は **%ポイント**（絶対値）で明記（相対%ではない）
- 例: P10が-5.20%から-4.50%に改善 = **+0.70%ポイント**の改善
- 例: 平均が3.50%から3.40%に低下 = **-0.10%ポイント**の低下

→ λを大きくしすぎて別の過学習をするリスクが減ります

**おすすめB：valで同じ目的関数を最大化（最も筋が良い）**

λは"ハイパーパラメータ"なので:
1. trainで最適化（各λ値でパラメータを最適化）
2. val（testの一部）で `mean + λ*min(0,p10)` を最大化するλを選ぶ
3. 最後に holdout（完全未使用、例：2025年だけ）で確認

**推奨分割例（ChatGPT推奨）**:
- train: 2020-01-01 ～ 2022-12-31（最適化用）
- val: 2023-01-01 ～ 2024-12-31（λ選定用）
- holdout: 2025-01-01 ～ 2025-12-31（最終確認用、完全未使用）

→ これが一番筋が良いです（現在の資料はtrain/testの2分割なので、最終確認用holdoutを明記すると完璧）

#### 推奨条件（可能な限り満たす）

4. **平均超過の減少幅**: 平均が大きく下がらない（例: **0.20%ポイント以内**の減少まで許容）
5. **勝率の向上**: λが大きくなるほど勝率が上がる（安定性の向上）

#### 理想的条件（あれば良い）

6. **最適化の安定性**: 複数回実行しても類似結果（seed違いで再実行）
   - **推奨（ChatGPT推奨）**: sampler seedを変えて2回（できれば3回）回し、平均/分散を見る
     * 実装例: `--random-seed 42`, `--random-seed 100`, `--random-seed 200`で3回実行
     * 最適化結果（best_value、best_params）の平均・分散を比較
     * これにより、λ選定がかなり信頼できる

#### P10の安定性に関する注意（ChatGPT推奨）

**問題**: P10はサンプル数が少ないとブレます。特に24Mはrb数が減るので、P10はほぼ「最悪値」に近づきがちです。

**24Mでは推奨（ChatGPT推奨）**:
- 24Mはtestサンプル数が少ない（12本程度）ので、**P10だけでλを決めない**
- **CVaR10（下位10%平均）**または**P20**を推奨（資料の将来案ではなく、24Mでは推奨として明記）
- これらにより、サンプル数が少ない場合でも安定した指標を得られる

**12Mの場合**:
- サンプル数が比較的多い（24-36本程度）ので、P10でも比較的安定
- ただし、`n_periods`を必ず出力して、サンプル数が少ないケースを検知する

**P10算出のサンプル数（n_periods）を必ず出力（ChatGPT推奨）**:
- train/testそれぞれで`n_periods`を表に載せる（24MでP10の信頼性が低いケースを一目で検知できる）

### 6.3 次のステップ

#### Step 1: 比較実行と結果分析
- [ ] `operational_24M`で4本のλ値を比較
- [ ] `12M_momentum`で4本のλ値を比較
- [ ] 結果を分析し、最適なλ値を選定

#### Step 2: 選定したλ値で再最適化
- [ ] 最適なλ値（例: 0.05）で`reoptimize_all_candidates.py`を実行
- [ ] パラメータファイルとレジストリを更新

#### Step 3: Step2/3の比較を再実行
- [ ] 新パラメータでStep2（A-1比較）を再実行
- [ ] Step3（rangeレジーム見直し）を再実行
- [ ] 結果を比較し、改善を確認

---

## 7. コードの主要箇所の参照

### 7.1 目的関数の実装
**ファイル**: `src/omanta_3rd/jobs/optimize_longterm.py`
- **関数**: `objective_longterm` (行533-775)
- **修正箇所**: 行747-757（下振れ罰の計算）

### 7.2 比較スクリプトの実装
**ファイル**: `src/omanta_3rd/jobs/compare_lambda_penalties.py`
- **関数**: `compare_lambda_penalties` (行245-441)
- **関数**: `run_backtest_with_params_file` (行115-242)
- **関数**: `save_comparison_results` (行444-488)

### 7.3 再最適化スクリプトの実装
**ファイル**: `src/omanta_3rd/jobs/reoptimize_all_candidates.py`
- **関数**: `optimize_and_save` (行186-285)
- **修正箇所**: `lambda_penalty`パラメータの追加

---

## 8. 質問と回答（想定）

### Q1: なぜ`min(0.0, p10_excess)`を使うのか？
**A**: P10が正の場合（下位10%でもプラス）は、下振れリスクがないためペナルティは不要です。P10が負の場合のみペナルティをかけるため、`min(0.0, p10_excess)`を使用しています。

### Q2: なぜ`objective_value = mean_excess + downside_penalty`なのか？
**A**: `downside_penalty`は常に負の値または0なので、実質的に減算されます。例えば、`mean_excess=3.5%`、`downside_penalty=-0.25%`の場合、`objective_value=3.5%+(-0.25%)=3.25%`となり、下振れリスクを考慮した値になります。

### Q3: 切替回数が常に0なのは問題ないか？
**A**: 今回は固定パラメータモードでの評価のため、切替回数は常に0です。レジーム切替戦略での切替回数が必要な場合は、別途実装が必要です（今回は対象外）。

### Q4: λ値をどのように選定すべきか？
**A**: 上記の「6.2 判断基準」を参照してください。基本的には、平均超過の減少幅とP10の改善度のバランスを考慮して選定します。

### Q5: 最適化時間が長くなるのは問題ないか？
**A**: 4本のλ値で並走実行するため、最適化時間が約4倍になります。`--n-trials`を減らすか、優先順位の高いλ値（例: 0.00, 0.05）だけを実行することも可能です。

---

## 9. まとめ

### 実装の要点
1. **目的関数の修正**: 平均超過だけでなく、下振れリスク（P10）も考慮
2. **複数λ値での比較**: 4本（0.00, 0.05, 0.03, 0.08）を並走実行し、結果を比較
3. **固定ホライズン評価**: 各ポートフォリオは`rebalance_date + horizon_months`まで評価
4. **train/test分割**: train期間で最適化、test期間で評価（過学習の検証）
5. **リバランス開始日の考慮**: 過去3年のデータ要件（データが2015年からある場合、2017-2018年からリバランス可能）

### 検証の重点（ChatGPT推奨事項を含む）
1. **実装の正確性**: 
   - 目的関数の計算（単位: %ポイント、符号の整合性）
   - 固定ホライズン評価（`performance_end_date == eval_end_date`、スナップ関数の引数がeval_dateであること）
   - train/test分割（`eval_end <= train_end_date`を満たすものだけをtrainに使用）
2. **結果の妥当性**: 
   - λ値の効果が現れているか、train/testギャップが適切か
   - P10算出のサンプル数（`n_periods`）を必ず出力して検証
   - 24MではP10ではなくCVaR10やP20を使用することを推奨
3. **最適なλ値の選定**: 
   - 平均超過の減少幅とP10の改善度のバランスを考慮（単位: %ポイントで明記）
   - 最適化のノイズ対策（sampler seedを変えて複数回実行、平均/分散を比較）
   - val/holdout分割を推奨（train/val/holdoutの3分割で、最終確認用holdoutを明記）

### 次のアクション
1. `operational_24M`と`12M_momentum`で比較実行
2. 結果を分析し、最適なλ値を選定
3. 選定したλ値で再最適化
4. Step2/3の比較を再実行し、改善を確認

---

以上です。ChatGPTの検証をお願いします。

